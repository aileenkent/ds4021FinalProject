Can we successfully predict a persons happiness level?

Our project proposal is worthwhile because in the growing age of social media, there are
increased concerns regarding impacts on mental health. There are many factors and habits that play a
role in a person’s happiness. Previous studies have included a connection with physical health, lack of
security of fulfillment of basic needs, and social isolation. Mental health is crucial to society and
struggles with it need to be addressed, but proposed solutions or methods for combating mental health
struggles need to be rooted in the variables that actually impact individuals the most. Therefore, it is
important to attempt to distinguish which factors contain more of a potentially significant role in order to
best be able to apply findings to aid people’s mental health struggles.

We found our dataset on Kaggle. The sample size is 500 entries, all being unique samples. There
are 10 columns, including age, gender, sleep quality, screen time, days without social media, happiness
index, and more. Our dataset is 50% male, 46% female, and 4% other. The participants range from 16 to
49 years old, with a mean age of 32.98 and a standard deviation of 9.96. To prepare our dataset for modeling, 
we had no missing values so we did not have to do anything in that regard. We did need to encode the categorical
variables, and we dropped the user ID column. 

We used four models: A Penalized Linear Model, Support Vector Machine, Ensemble Model, and a Neural Network.
All models required the same preprocessing steps which are label encoding categorical variables and standardizing 
features using StandardScaler. Elastic Net combines L1 and L2 regularization for feature selection, with hyperparameters 
tuned over alpha values [0.001, 0.01, 0.1, 0.5, 1, 5, 10] and l1_ratio [0.1, 0.3, 0.5, 0.7, 0.9]. SVMs capture
non-linear relationships through kernel transformations, while the Ensemble Model used Random Forest. 
The Neural Network used fully connected layers with ReLU activation and dropout regularization, trained with SGD 
optimization. All models except for Neural Network used 5-fold cross-validation to estimate generalization performance 
by partitioning training data into 5 equal subsets, with each fold serving as validation while the remaining folds were
used for training. The Neural Network model uses 3-fold validation. Model performance was evaluated using Mean Squared Error 
(MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and R² Score to assess prediction accuracy and model reliability.


Our machine learning analysis yielded significant performance differences among the four models evaluated through cross-validation.
The table below summarizes the cross-validated performance metrics for the optimal hyperparameter combination of each model.
Compared to the other models, the Random Forest model demonstrated the strongest performance during the optimization process.
This model achieved the lowest mean squared error (MSE = 0.2568) and highest R-squared value (R² = 0.8921) on the training data.

### Cross-Validated Performance of Optimized Models on Training Data

| Model Type                  | MSE    | R²     | Best Hyperparameters                                                    |
|-----------------------------|--------|--------|-------------------------------------------------------------------------|
| ElasticNet Regression       | 0.8804 | 0.6300 | α=0.01, L1 ratio=0.9                                                    |
| Support Vector Machine      | 0.8542 | 0.6410 | C=0.1, kernel='line                                                     |
| Random Forest               | 0.2568 | 0.8921 | n_estimators=200, max_depth=10, min_samples_leaf=2, min_samples_split=5 |
| Neural Network (PyTorch)    | 0.9409 | 0.5885 | hidden_dim=16, learning_rate=0.01, batch_size=16, n_epochs=300          |

Based on its cross-validation performance, we selected the Random Forest model with optimal hyperparameters (n_estimators=200,
max_depth=10, min_samples_split=5, min_samples_leaf=2) to evaluate  on the test set. The model achieved a test MSE of 0.7972,
RMSE of 0.8929, R² of 0.6060, and MAE of 0.6621. Our model produced significantly degraded performance metrics when predicting
on the test set. This decrease in performance is indicative of some overfitting in the model. It suggests that the model is
capturing some of the patterns that allow it to generalize to unseen data, but is becoming too specific to the training data.
Despite this, the model maintains some explanatory power. Its R² value indicates that the model can explain ~60% of the
variance in the data.
